{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dc4a3-468e-444b-b8f3-71b6b47f12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.patches as mpatches\n",
    "#make figures bigger on HiDPI monitors\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "import scipy.io as io\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def tfp_function_factory(model, *args):\n",
    "    \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    Based on the example from https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        *args: arguments to be passed to model.get_grads method        \n",
    "        \n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.prod(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # now create a function that will be returned by this factory\n",
    "    @tf.function\n",
    "    def f(params_1d):\n",
    "        \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "\n",
    "        This function is created by function_factory.\n",
    "\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "        assign_new_model_parameters(params_1d)        \n",
    "        loss_value, grads = model.get_grad(*args)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "        if f.iter%model.print_epoch == 0:\n",
    "            tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        return loss_value, grads\n",
    "\n",
    "    # store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "\n",
    "    return f\n",
    "\n",
    "class CalculateUPhi4th_DD(tf.keras.Model): \n",
    "    def __init__(self, layers_list, train_op, num_epoch, print_epoch,model_data,data_type):\n",
    "        super().__init__()\n",
    "        self.model_layers_list = layers_list\n",
    "        self.train_op = train_op\n",
    "        self.num_epoch = num_epoch\n",
    "        self.print_epoch = print_epoch\n",
    "        self.data_type = data_type\n",
    "        self.l = model_data['l']\n",
    "        self.B = model_data['B']\n",
    "        self.Gc = model_data['Gc']\n",
    "        self.E = model_data['E']\n",
    "        self.crack_loc = model_data['crack_loc']\n",
    "\n",
    "    #@tf.function                                \n",
    "    def call(self, X_all):\n",
    "        output =[]\n",
    "        for i in range(len(X_all)):\n",
    "            u_val, phi_val = self.u_phi(X_all[i][:,0:1],i)\n",
    "            output.append(tf.concat([u_val,phi_val],1))\n",
    "        return output \n",
    "    \n",
    "    def dirichletBound(self, X, xPhys, net_indx):\n",
    "        u_val = (xPhys+1)*(xPhys-1)*X[:, 0:1]\n",
    "        return u_val\n",
    "\n",
    "   \n",
    "    def net_hist(self,x):        \n",
    "        init_hist = tf.zeros_like(x)\n",
    "        dist = tf.abs(x-self.crack_loc)\n",
    "        init_hist = tf.where(dist < 0.5*self.l, self.B*self.Gc*0.5*(1-(2*dist/self.l))/self.l, init_hist)        \n",
    "        return init_hist  \n",
    "    \n",
    "    def net_energy(self,x, net_indx):\n",
    "\n",
    "        u, phi = self.u_phi(x, net_indx)        \n",
    "        hist = self.net_hist(x)\n",
    "        \n",
    "        g = (1-phi)**2\n",
    "        u_x, phi_x = self.du_phi(x, net_indx)\n",
    "        nabla = phi_x**2\n",
    "        _, phi_xx = self.d2u_phi(x, net_indx)\n",
    "        laplacian = phi_xx**2        \n",
    "        sigmaX = self.E*u_x \n",
    "        \n",
    "        energy_u = 0.5*g*sigmaX*u_x\n",
    "        energy_phi = 0.5*self.Gc * (phi**2/self.l + self.l*nabla + 0.5*self.l**3*laplacian) + g*hist        \n",
    "        return energy_u, energy_phi\n",
    "\n",
    "    # Running the model\n",
    "    #@tf.function\n",
    "    def u_phi(self,X, net_indx):\n",
    "        xPhys = X\n",
    "        X = 2.0*(X - self.bounds[\"lb\"][net_indx])/(self.bounds[\"ub\"][net_indx] - self.bounds[\"lb\"][net_indx]) - 1.0\n",
    "        for l in self.model_layers_list[net_indx]:\n",
    "            X = l(X)\n",
    "        u = self.dirichletBound(X, xPhys, net_indx)\n",
    "        phi = X[:, 1:2]\n",
    "        return u, phi\n",
    "                \n",
    "    # Return the first derivatives\n",
    "    def du_phi(self, xPhys, net_indx):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(xPhys)\n",
    "            u_val,phi_val = self.u_phi(xPhys, net_indx)\n",
    "        du_val = tape.gradient(u_val, xPhys)\n",
    "        dphi_val = tape.gradient(phi_val, xPhys)\n",
    "        return du_val, dphi_val\n",
    "    \n",
    "    # Return the second derivative\n",
    "    def d2u_phi(self, xPhys, net_indx):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(xPhys)\n",
    "            du_val, dphi_val = self.du_phi(xPhys, net_indx)\n",
    "        d2u_val = tape.gradient(du_val, xPhys)\n",
    "        d2phi_val = tape.gradient(dphi_val, xPhys)\n",
    "        return d2u_val,d2phi_val\n",
    "        \n",
    "    #Custom loss function\n",
    "    @tf.function\n",
    "    def get_all_losses(self, Xint_all, Wint_all, Yint_all,\n",
    "                       Xif_all, Yif_all, if_list):\n",
    "        # calculate the losses for energy, phi and external work\n",
    "        loss_energy_u = 0.\n",
    "        loss_energy_phi = 0.\n",
    "        loss_ext_work = 0. \n",
    "        for i in range(len(Xint_all)):\n",
    "            xPhys_int = Xint_all[i][:,0:1]\n",
    "            yPhys_int = Yint_all[i][:,0:1]\n",
    "            u, _ = self.u_phi(xPhys_int,i)\n",
    "            energy_u, energy_phi = self.net_energy(xPhys_int,i)\n",
    "            loss_energy_u += tf.reduce_sum(energy_u*Wint_all[i])\n",
    "            loss_energy_phi += tf.reduce_sum(energy_phi*Wint_all[i])\n",
    "            loss_ext_work += tf.reduce_sum(u*yPhys_int*Wint_all[i])  \n",
    "\n",
    "        # compute the interface loss and interior loss along the interface\n",
    "        iface_loss = 0.\n",
    "        for i in range(len(Xif_all)):\n",
    "            indx_a = if_list[i][0]\n",
    "            indx_b = if_list[i][1]\n",
    "            xPhys = Xif_all[i][:, 0:1]                                                  \n",
    "            u_a, phi_a = self.u_phi(xPhys, indx_a)\n",
    "            u_b, phi_b = self.u_phi(xPhys, indx_b)\n",
    "            dudx_a, dphidx_a = self.du_phi(xPhys, indx_a)\n",
    "            dudx_b, dphidx_b = self.du_phi(xPhys, indx_b)\n",
    "            iface_loss += tf.reduce_mean(tf.math.square(u_a-u_b))\n",
    "            iface_loss += tf.reduce_mean(tf.math.square(phi_a-phi_b))\n",
    "            iface_loss += tf.reduce_mean(tf.math.square(dudx_a-dudx_b))\n",
    "            iface_loss += tf.reduce_mean(tf.math.square(dphidx_a-dphidx_b))\n",
    "\n",
    "        return loss_energy_u, loss_energy_phi, loss_ext_work, iface_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def get_loss(self,Xint_all, Wint_all,Yint_all, Xif_all, Yif_all, if_list):\n",
    "        loss_energy_u, loss_energy_phi, loss_ext_work, iface_loss = self.get_all_losses(Xint_all, Wint_all,\n",
    "                                                Yint_all, Xif_all, Yif_all, if_list)\n",
    "        return loss_energy_u + loss_energy_phi + iface_loss - loss_ext_work\n",
    "          \n",
    "    # get gradients\n",
    "    @tf.function\n",
    "    def get_grad(self,Xint_all, Wint_all,Yint_all, Xif_all, Yif_all, if_list):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            L = self.get_loss(Xint_all, Wint_all,Yint_all, Xif_all, Yif_all, if_list)\n",
    "        g = tape.gradient(L, self.trainable_variables)\n",
    "        return L, g    \n",
    "      \n",
    "    # perform gradient descent\n",
    "    def network_learn(self, Xint_all, Wint_all,Yint_all, Xif_all, Yif_all, if_list):\n",
    "        self.bounds = {\"lb\": [], \"ub\": []}\n",
    "        for i in range(len(Xint_all)):\n",
    "            xmin = tf.math.reduce_min(Xint_all[i][:,0])\n",
    "            xmax = tf.math.reduce_max(Xint_all[i][:,0])\n",
    "            self.bounds[\"lb\"].append(xmin)\n",
    "            self.bounds[\"ub\"].append(xmax)\n",
    "        for i in range(self.num_epoch):\n",
    "            L, g = self.get_grad(Xint_all, Wint_all,Yint_all, Xif_all, Yif_all, if_list)\n",
    "            self.train_op.apply_gradients(zip(g, self.trainable_variables))\n",
    "            if i%self.print_epoch==0:\n",
    "                loss_energy_u, loss_energy_phi, loss_ext_work, iface_loss = self.get_all_losses(Xint_all, \n",
    "                                Wint_all,Yint_all, Xif_all, Yif_all, if_list)\n",
    "                L = loss_energy_u + loss_energy_phi + iface_loss - loss_ext_work\n",
    "                print(\"Epoch {} loss: {}, loss_energy_u: {}, loss_energy_phi: {}, iface_loss: {}\".format(i, \n",
    "                                        L, loss_energy_u, loss_energy_phi,iface_loss))  \n",
    "                    \n",
    "def loading_fun(x):\n",
    " \n",
    "    y = np.sin(np.pi*x)\n",
    "    return y\n",
    "\n",
    "def exact_sol(x, l):\n",
    "    \n",
    "    u_exact = np.sin(np.pi*x)/(np.pi)**2 + np.where(x < 0.0,\n",
    "                                    -1.0*(1+x)/np.pi, (1-x)/np.pi)\n",
    "    phi_exact = np.exp(-np.absolute(x-0.)/l)*(1+np.absolute(x-0.)/l)\n",
    "    \n",
    "    return u_exact, phi_exact\n",
    "\n",
    "\n",
    "xmin = -1.\n",
    "xmid = 0.\n",
    "xmax = 1.\n",
    "dom1 = np.array([-1.0, 0])\n",
    "dom2 = np.array([0, 1.0])\n",
    "   \n",
    "\n",
    "data_type = \"float64\"\n",
    "data = io.loadmat('coordinates.mat')\n",
    "\n",
    "sys.exit()\n",
    "# plt.scatter(Xint, np.zeros_like(Xint), s=0.5, color='black')\n",
    "\n",
    "model_data = dict()\n",
    "model_data['E'] = 1.\n",
    "model_data['l'] = 0.0125\n",
    "model_data['B'] = 1000.\n",
    "model_data['Gc'] = 2.7\n",
    "model_data['crack_loc'] = 0.\n",
    "\n",
    "#define the model \n",
    "tf.keras.backend.set_floatx(data_type)\n",
    "l1 = tf.keras.layers.Dense(10, \"swish\")\n",
    "l12 = tf.keras.layers.Dense(10, \"swish\")\n",
    "l13 = tf.keras.layers.Dense(10, \"swish\")\n",
    "l1_out = tf.keras.layers.Dense(2, None)\n",
    "\n",
    "l2 = tf.keras.layers.Dense(10, \"swish\")\n",
    "l22 = tf.keras.layers.Dense(10, \"swish\")\n",
    "l23 = tf.keras.layers.Dense(10, \"swish\")\n",
    "l2_out = tf.keras.layers.Dense(2, None)\n",
    "\n",
    "train_op = tf.keras.optimizers.Adam()\n",
    "num_epoch = 20000\n",
    "print_epoch = 100\n",
    "pred_model = CalculateUPhi4th_DD([[l1, l12, l13, l1_out], [l2, l22, l23, l2_out]], train_op,\n",
    "                                      num_epoch, print_epoch, model_data, data_type)                              \n",
    "\n",
    "#convert the training data to tensors\n",
    "Xint1_tf = tf.convert_to_tensor(data['Xint_1'])\n",
    "Xint2_tf = tf.convert_to_tensor(data['Xint_2'])\n",
    "\n",
    "Wint1_tf = tf.convert_to_tensor(data['wgtsPhys1'].astype(data_type))\n",
    "Wint2_tf = tf.convert_to_tensor(data['wgtsPhys2'].astype(data_type))\n",
    "\n",
    "Yint1_tf = tf.convert_to_tensor(data['Yint_1'])\n",
    "Yint2_tf = tf.convert_to_tensor(data['Yint_2'])\n",
    "\n",
    "\n",
    "Xint_all = [Xint1_tf, Xint2_tf]\n",
    "Wint_all = [Wint1_tf, Wint2_tf]\n",
    "Yint_all = [Yint1_tf, Yint2_tf]\n",
    "\n",
    "Xif_12 = np.array([[0]]).astype(data_type)\n",
    "if_list = [(0,1)]\n",
    "Yif_12 = loading_fun(Xif_12).astype(data_type)\n",
    "\n",
    "Xif_12 = [tf.convert_to_tensor(Xif_12)]\n",
    "Yif_12 = [tf.convert_to_tensor(Yif_12)]\n",
    "\n",
    "t0 = time.time()\n",
    "print(\"Training (ADAM)...\")\n",
    "\n",
    "pred_model.network_learn(Xint_all, Wint_all, Yint_all, Xif_12, Yif_12, if_list)\n",
    "t1 = time.time()\n",
    "print(\"Time taken (ADAM)\", t1-t0, \"seconds\")\n",
    "\n",
    "print(\"Training (TFP-BFGS)...\")\n",
    "\n",
    "loss_func = tfp_function_factory(pred_model, Xint_all, Wint_all, Yint_all, Xif_12, Yif_12, if_list)\n",
    "# convert initial model parameters to a 1D tf.Tensor\n",
    "init_params = tf.dynamic_stitch(loss_func.idx, pred_model.trainable_variables)\n",
    "# train the model with BFGS solver\n",
    "results = tfp.optimizer.bfgs_minimize(\n",
    "value_and_gradients_function=loss_func, initial_position=init_params,\n",
    "      max_iterations=1000, tolerance=1e-14)\n",
    "# after training, the final optimized parameters are still in results.position\n",
    "# so we have to manually put them back to the model\n",
    "loss_func.assign_new_model_parameters(results.position)\n",
    "t2 = time.time()\n",
    "print(\"Time taken (BFGS)\", t2-t1, \"seconds\")\n",
    "print(\"Time taken (all)\", t2-t0, \"seconds\")\n",
    "\n",
    "\n",
    "print(\"Testing...\")\n",
    "numPtsTest = 300\n",
    "x1_test = np.linspace(xmin, xmid, numPtsTest).astype(data_type)\n",
    "x1_test = np.array(x1_test)[np.newaxis].T\n",
    "x1_tf = tf.convert_to_tensor(x1_test)\n",
    "\n",
    "x2_test = np.linspace(xmid, xmax, numPtsTest).astype(data_type)\n",
    "x2_test = np.array(x2_test)[np.newaxis].T\n",
    "x2_tf = tf.convert_to_tensor(x2_test)\n",
    "\n",
    "XTest_all = [x1_tf, x2_tf]\n",
    "YTest_all = pred_model(XTest_all)\n",
    "\n",
    "# splitting the u and phi components \n",
    "UTest_all = []\n",
    "PTest_all = []\n",
    "for i in range(len(YTest_all)):\n",
    "    UTest_all.append(YTest_all[i][:, 0:1])\n",
    "    PTest_all.append(YTest_all[i][:, 1:2])\n",
    "\n",
    "# u_exact, phi_exact = exact_sol(x_test, model_data['l'])\n",
    "\n",
    "err_all_u = []\n",
    "err_all_phi = []\n",
    "err_norm_u  = 0.\n",
    "err_norm_phi = 0.\n",
    "exact_norm_u = 0.\n",
    "exact_norm_phi = 0.\n",
    "for i in range(len(YTest_all)):\n",
    "    u_exact, phi_exact = exact_sol(XTest_all[i][:,0:1], model_data['l'])\n",
    "    u_test = UTest_all[i]\n",
    "    phi_test = PTest_all[i]\n",
    "    err_all_u.append(u_exact - u_test)\n",
    "    err_all_phi.append(phi_exact - phi_test)\n",
    "    err_norm_u += np.sum((u_exact - u_test)**2)\n",
    "    err_norm_phi += np.sum((phi_exact - phi_test)**2)\n",
    "    exact_norm_u += np.sum(u_exact**2)\n",
    "    exact_norm_phi += np.sum(phi_exact**2)\n",
    "rel_err_u = np.sqrt(err_norm_u/exact_norm_u)\n",
    "rel_err_phi = np.sqrt(err_norm_phi/exact_norm_phi)\n",
    "print(\"Relative error u: \", rel_err_u)\n",
    "print(\"Relative error phi: \", rel_err_phi)\n",
    "\n",
    "plt.plot(x1_test, UTest_all[0], x2_test, UTest_all[1], label = 'u_pred')\n",
    "plt.plot(x1_test, PTest_all[0], x2_test, PTest_all[1], label = 'phi_pred')\n",
    "\n",
    "x12_test = np.concatenate((x1_test, x2_test), axis=0)\n",
    "u_exact, phi_exact = exact_sol(x12_test, model_data['l'])\n",
    "\n",
    "plt.plot(x12_test, u_exact, '--', label = 'u_exact')\n",
    "plt.plot(x12_test, phi_exact, '--', label = 'phi_exact')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
